<!DOCTYPE html><html><head><meta charset="utf-8"><title>Sharding data across service fabric partitions | Naeem Khedarun</title><meta name="author" content="Naeem Khedarun"><meta name="description" content="Service fabric gives you two mechanisms out of the box when resolving which partition you hit when calling a Reliable Service. We’ll ignore the single"><meta name="viewport" content="width=device-width,initial-scale=1"><meta property="og:title" content="Sharding data across service fabric partitions"><meta property="og:site_name" content="Naeem Khedarun"><meta property="og:image" content="http://naeem.khedarun.co.uk/blog/images/code-cover.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png"><link rel="icon" type="image/png" sizes="96x96" href="/images/favicon-96.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png"><link rel="alternate" href="/blog/atom.xml" title="Naeem Khedarun" type="application/atom+xml"><link rel="stylesheet" href="/css/all-22b259cd94.min.css"><script type="text/javascript" src="https://www.google.com/jsapi?autoload={'modules':[{'name':'visualization','version':'1','packages':['corechart']}]}"></script></head><body><div id="wrap"><div class="navbar navbar-static-top" role="navigation"><div class="container"><div class="col-12 navbar-center"><a href="/blog/"><span class="name-brand">Naeem Khedarun</span></a> <span class="navbar-spacer"></span> <span><a href="https://twitter.com/naeemkhedarun"><i class="fa fa-twitter"></i></a> <a href="https://github.com/naeemkhedarun"><i class="fa fa-github"></i></a> <a href="https://uk.linkedin.com/in/naeemkhedarun"><i class="fa fa-linkedin-square"></i></a> <a href="/blog/atom.xml"><i class="fa fa-rss"></i></a></span></div></div></div><div class="container"><div class="row"><div class="col-12 col-md-12"><h1 class="title">Sharding data across service fabric partitions</h1><i>21 hours ago</i><p></p><p>Service fabric gives you two mechanisms out of the box when resolving which partition you hit when calling a Reliable Service. We’ll ignore the singleton partitions as they won’t help us with sharding.</p><ul><li><strong>Named Partition</strong> - This is a fixed name for each partition configured at deploy time.</li><li><strong>Ranged Partition</strong> - This uses an <code>Int64</code> range to decide which partition a numbered key falls in.</li></ul><p>More information can be found <a href="https://azure.microsoft.com/en-gb/documentation/articles/service-fabric-concepts-partitioning/" target="_blank" rel="external">here</a>.</p><h3 id="Named_Partitioning">Named Partitioning</h3><p>A named partition allows you to specify explicitly which partition you want to access at runtime. A common example is to specify A-Z named partitions and use the first letter of your data as the key. This splits your data into 26 partitions.</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">Service</span> <span class="attribute">Name</span>=<span class="value">"TestService"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">StatefulService</span> <span class="attribute">ServiceTypeName</span>=<span class="value">"TestServiceType"</span> </span><br><span class="line">                   <span class="attribute">TargetReplicaSetSize</span>=<span class="value">"3"</span> </span><br><span class="line">                   <span class="attribute">MinReplicaSetSize</span>=<span class="value">"2"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">NamedPartition</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">Partition</span> <span class="attribute">Name</span>=<span class="value">"a"</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">Partition</span> <span class="attribute">Name</span>=<span class="value">"b"</span>/&gt;</span></span><br><span class="line">      ...</span><br><span class="line">      <span class="tag">&lt;<span class="title">Partition</span> <span class="attribute">Name</span>=<span class="value">"z"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">NamedPartition</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">StatefulService</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">Service</span>&gt;</span></span><br></pre></td></tr></table></figure><p>The advantages to this are that it is simple and you know which partition your data goes in without a lookup. Unfortunately as we will test later, you are unlikely to get a good distribution of your data across the partitions.</p><h3 id="Ranged_Partitioning">Ranged Partitioning</h3><p>With a ranged partition the fabric tooling by default uses the entire <code>Int64</code> range as keys to decide which partition. It will then convert these into ranges or buckets depending on the partition count.</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">Service</span> <span class="attribute">Name</span>=<span class="value">"TestService"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">StatefulService</span> <span class="attribute">ServiceTypeName</span>=<span class="value">"TestServiceType"</span> </span><br><span class="line">                   <span class="attribute">TargetReplicaSetSize</span>=<span class="value">"3"</span> </span><br><span class="line">                   <span class="attribute">MinReplicaSetSize</span>=<span class="value">"2"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">UniformInt64Partition</span> <span class="attribute">PartitionCount</span>=<span class="value">"26"</span></span><br><span class="line">                           <span class="attribute">LowKey</span>=<span class="value">"-9223372036854775808"</span> </span><br><span class="line">                           <span class="attribute">HighKey</span>=<span class="value">"9223372036854775807"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">StatefulService</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">Service</span>&gt;</span></span><br></pre></td></tr></table></figure><p>However to be able to lookup a partition we need a function which can reduce our data to an integer value. To use the configuration above we can convert our strings into an <code>Int64</code>.</p><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> md5 = MD5.Create();</span><br><span class="line"><span class="keyword">var</span> <span class="keyword">value</span> = md5.ComputeHash(Encoding.ASCII.GetBytes(<span class="keyword">value</span>));</span><br><span class="line"><span class="keyword">var</span> key = BitConverter.ToInt64(<span class="keyword">value</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> client = ServiceProxy.Create&lt;ITestService&gt;(</span><br><span class="line">                    key, </span><br><span class="line">                    <span class="keyword">new</span> Uri(<span class="string">"fabric:/App/TestService"</span>))</span><br></pre></td></tr></table></figure><ol><li>Hash the value to a fixed length byte array.</li><li>Convert the array to an <code>Int64</code>.</li><li>Create the client with the calculated key to connect to the service on that partition.</li></ol><h3 id="Ranged_Partition_with_Consistent_Hashing">Ranged Partition with Consistent Hashing</h3><p>Rather than use the ranges, you can fix your keys and plug in your own hash algorithm to resolve the partition.</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">Service</span> <span class="attribute">Name</span>=<span class="value">"TestService"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">StatefulService</span> <span class="attribute">ServiceTypeName</span>=<span class="value">"TestServiceType"</span> </span><br><span class="line">                   <span class="attribute">TargetReplicaSetSize</span>=<span class="value">"3"</span> </span><br><span class="line">                   <span class="attribute">MinReplicaSetSize</span>=<span class="value">"2"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">UniformInt64Partition</span> <span class="attribute">PartitionCount</span>=<span class="value">"26"</span> </span><br><span class="line">                           <span class="attribute">LowKey</span>=<span class="value">"0"</span> </span><br><span class="line">                           <span class="attribute">HighKey</span>=<span class="value">"25"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">StatefulService</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">Service</span>&gt;</span></span><br></pre></td></tr></table></figure><p>We now have a key range limited to 0-25 rather than the entire <code>Int64</code> range. We can resolve a client connected to this partition in the same way, however this time we need to compute a key that fits in this smaller range. I’m using the jump consistent hash implementation in <a href="https://github.com/turowicz/Hydra" target="_blank" rel="external">hydra</a>.</p><figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> shard = <span class="keyword">new</span> JumpSharding().GetShard(<span class="keyword">value</span>, <span class="number">26</span>);</span><br><span class="line"><span class="keyword">var</span> client = ServiceProxy.Create&lt;ITestService&gt;(</span><br><span class="line">                    shard, </span><br><span class="line">                    <span class="keyword">new</span> Uri(<span class="string">"fabric:/App/TestService"</span>))</span><br></pre></td></tr></table></figure><ol><li>Call get shard with the value and number of partitions to distribute across.</li><li>Create the client with the calculated key to connect to the service on that partition.</li></ol><h3 id="Distribution">Distribution</h3><p>To benchmark the distribution we have a list of around 17000 real email addresses. This should give us an idea of how the sharding strategies will distribute the data across 26 partitions. Another advantage of using one of the <code>Int64</code> methods is that they can be used with any amount of partitions.</p><p>We are looking for an even number of accounts allocated to each partition.</p><chart type="BarChart" options="{'title':'Email accounts distribution comparison','vAxis':{'title':'Partitions'},'legend':{'position':'bottom'}, 'height':'750'}"><br><div></div><br></chart><table><thead><tr><th>Partition</th><th>Alphabet</th><th>Consistent Hash</th><th>Ranging</th></tr></thead><tbody><tr><td>0</td><td>1569</td><td>684</td><td>650</td></tr><tr><td>1</td><td>912</td><td>682</td><td>730</td></tr><tr><td>2</td><td>1027</td><td>647</td><td>646</td></tr><tr><td>3</td><td>1175</td><td>662</td><td>701</td></tr><tr><td>4</td><td>513</td><td>687</td><td>700</td></tr><tr><td>5</td><td>415</td><td>665</td><td>658</td></tr><tr><td>6</td><td>581</td><td>653</td><td>684</td></tr><tr><td>7</td><td>466</td><td>693</td><td>637</td></tr><tr><td>8</td><td>405</td><td>657</td><td>690</td></tr><tr><td>9</td><td>1714</td><td>681</td><td>699</td></tr><tr><td>10</td><td>643</td><td>654</td><td>669</td></tr><tr><td>11</td><td>608</td><td>696</td><td>681</td></tr><tr><td>12</td><td>1800</td><td>734</td><td>665</td></tr><tr><td>13</td><td>526</td><td>717</td><td>647</td></tr><tr><td>14</td><td>213</td><td>693</td><td>613</td></tr><tr><td>15</td><td>793</td><td>693</td><td>676</td></tr><tr><td>16</td><td>31</td><td>654</td><td>683</td></tr><tr><td>17</td><td>1039</td><td>681</td><td>713</td></tr><tr><td>18</td><td>1562</td><td>661</td><td>665</td></tr><tr><td>19</td><td>803</td><td>708</td><td>747</td></tr><tr><td>20</td><td>46</td><td>653</td><td>709</td></tr><tr><td>21</td><td>268</td><td>693</td><td>666</td></tr><tr><td>22</td><td>301</td><td>678</td><td>679</td></tr><tr><td>23</td><td>55</td><td>702</td><td>675</td></tr><tr><td>24</td><td>134</td><td>670</td><td>708</td></tr><tr><td>25</td><td>136</td><td>737</td><td>744</td></tr></tbody></table><p>We can see from those results that sharding using the first character of an email produces wildly different partition sizes, not what we want! Both the jump hash and integer ranging methods produced very even parition sizes.</p><h3 id="Conclusion">Conclusion</h3><p>Based on these results I would use the ranged partitioning method, it produces provides good balancing and is fast to compute. An additional advantage is you do not need to know the partition count in the code, just map your data to an <code>Int64</code> and service fabric will do the rest.</p><div class="categories"><span>Posted in:</span><a href="/blog/categories/development/">development</a></div><div class="tags"><span>Tagged with:</span><a href="/blog/tags/NET/">.NET</a>, <a href="/blog/tags/C/">C#</a>, <a href="/blog/tags/azure/">azure</a>, <a href="/blog/tags/servicefabric/">servicefabric</a></div><br><hr><div class="row"><div class="col-md-12"><h3>Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div></div></div></div><div id="push"></div></div><div id="footer"><div class="container"><div class="container"><br><p class="muted credit">&copy; 2016 Naeem Khedarun</p></div></div></div><script src="/js/all-4c3c81d67b.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-71815871-1', 'auto');
ga('send', 'pageview');</script><script type="text/javascript">var disqus_shortname = 'naeemkhedarun';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());</script></body></html>