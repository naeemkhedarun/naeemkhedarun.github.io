<!DOCTYPE html><html><head><meta charset="utf-8"><title>Azure Networking Bandwidth: Public IP vs Peered Networking | Naeem Khedarun</title><meta name="author" content="Naeem Khedarun"><meta name="description" content="We have a application setup which might be familiar to you; A cloud service in a classic virtual network (v1) which communicates with a database in an"><meta name="viewport" content="width=device-width,initial-scale=1"><meta property="og:title" content="Azure Networking Bandwidth: Public IP vs Peered Networking"><meta property="og:site_name" content="Naeem Khedarun"><meta property="og:image" content="http://naeem.khedarun.co.uk/blog/images/code-cover.png"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://naeem.khedarun.co.uk/blog/images/code-cover.png"><meta property="og:url" content="http://naeem.khedarun.co.uk/blog/blog/2016/10/16/Azure-Networking-Bandwidth-Public-IP-vs-Peered-Networking-1476657686312/"><meta name="twitter:site" content="@NaeemKhedarun"><meta name="twitter:creator" content="@NaeemKhedarun"><meta name="twitter:title" content="Azure Networking Bandwidth: Public IP vs Peered Networking"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32.png"><link rel="icon" type="image/png" sizes="96x96" href="/images/favicon-96.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.png"><link rel="alternate" href="/blog/atom.xml" title="Naeem Khedarun" type="application/atom+xml"><link rel="stylesheet" href="/css/all-79248e3d39.min.css"><script type="text/javascript" src="https://www.google.com/jsapi?autoload={'modules':[{'name':'visualization','version':'1','packages':['corechart']}]}"></script></head><body><div id="wrap"><div class="navbar navbar-static-top" role="navigation"><div class="container"><div class="col-12 navbar-center"><a href="/blog/"><span class="name-brand">Naeem Khedarun</span></a> <span class="navbar-spacer"></span> <span><a href="https://twitter.com/naeemkhedarun"><i class="fa fa-twitter"></i></a> <a href="https://github.com/naeemkhedarun"><i class="fa fa-github"></i></a> <a href="https://uk.linkedin.com/in/naeemkhedarun"><i class="fa fa-linkedin-square"></i></a> <a href="/blog/atom.xml"><i class="fa fa-rss"></i></a></span></div></div></div><div class="container"><div class="row"><div class="col-12 col-md-12"><h1 class="title">Azure Networking Bandwidth: Public IP vs Peered Networking</h1><i>9 hours ago</i><p></p><p>We have a application setup which might be familiar to you; A cloud service in a classic virtual network (v1) which communicates with a database in an ARM virtual network (v2). Ideally we would like both of these services in a single network, but are restricted from doing so due to the deployment models. We had a discussion which involved performance, security and ideal topologies, however this post will solely focus on performance.</p><blockquote><p>Is there a difference in latency and bandwidth when they are both hosted in the same region?</p></blockquote><h3 id="Test-setup"><a href="#Test-setup" class="headerlink" title="Test setup"></a>Test setup</h3><p>To reflect the setup we have for our application, two VMs were provisioned in North Europe.</p><p><strong>Source</strong></p><ul><li>A3 (Large) Windows Cloud service</li><li>Classic Virtual Network</li></ul><p><strong>Destination</strong></p><ul><li>DS13v2 Linux Virtual machine</li><li>ARM Virtual Network peered to the Classic VNet</li></ul><h3 id="Traceroute"><a href="#Traceroute" class="headerlink" title="Traceroute"></a>Traceroute</h3><p>I first wanted to test the latency and number of hops between the VMs. ICMP is not available for this test as we are hitting a public IP, however we can use TCP by using <a href="https://blogs.msdn.microsoft.com/gsamant/2015/02/16/ping-and-tracert-commands-on-azure-vm/" target="_blank" rel="external">nmap</a>.</p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PS C:\Users\user&gt; nmap -sS -Pn -p 80 --traceroute 13.xx.xx.xx</div><div class="line"></div><div class="line">HOP RTT     ADDRESS</div><div class="line">1   ... 7</div><div class="line">8   0.00 ms 13.xx.xx.xx</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PS C:\Users\user&gt; nmap -sS -Pn -p 80 --traceroute 10.xx.xx.xx</div><div class="line"></div><div class="line">HOP RTT     ADDRESS</div><div class="line">1   0.00 ms 10.xx.xx.xx</div></pre></td></tr></table></figure><p>We can see that there are 8 hops over the public IP, and as we expect only a single hop over the peered network. Both routes are still extremely fast with negligible ping times. This confirms my collegues suspicions; despite connecting to a public address the traffic probably never leaves the datacenters perimeter network.</p><h3 id="Bandwidth-iperf3"><a href="#Bandwidth-iperf3" class="headerlink" title="Bandwidth (iperf3)"></a>Bandwidth (iperf3)</h3><p>To measure the bandwidth available between the VMs Iâ€™m using <a href="https://iperf.fr/iperf-download.php" target="_blank" rel="external">iperf3</a> which is cross platform. The test is run from the windows machine as a client and flows to the iperf server hosted on the linux box.</p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment"># Public IP test</span></div><div class="line">.\iperf3.exe -c 13.xx.xx.xx -i 1 -t 30</div><div class="line"></div><div class="line"><span class="comment"># Peered network test</span></div><div class="line">.\iperf3.exe -c 10.xx.xx.xx -i 1 -t 30</div></pre></td></tr></table></figure><chart type="LineChart" options="{'legend':{'position':'bottom'}, 'height':'300', 'vAxis': { 'minValue': 0, 'title':'Bandwidth (Mbit)'}, 'hAxis':{'title':'Time (s)'}}"><br><div></div><br></chart><table><thead><tr><th>Seconds</th><th>Public IP</th><th>Peered</th></tr></thead><tbody><tr><td>1</td><td>985</td><td>996</td></tr><tr><td>2</td><td>951</td><td>947</td></tr><tr><td>3</td><td>975</td><td>976</td></tr><tr><td>4</td><td>936</td><td>956</td></tr><tr><td>5</td><td>989</td><td>962</td></tr><tr><td>6</td><td>958</td><td>965</td></tr><tr><td>7</td><td>967</td><td>962</td></tr><tr><td>8</td><td>959</td><td>926</td></tr><tr><td>9</td><td>964</td><td>985</td></tr><tr><td>10</td><td>961</td><td>948</td></tr><tr><td>11</td><td>968</td><td>953</td></tr><tr><td>12</td><td>960</td><td>980</td></tr><tr><td>13</td><td>949</td><td>957</td></tr><tr><td>14</td><td>976</td><td>966</td></tr><tr><td>15</td><td>960</td><td>949</td></tr><tr><td>16</td><td>966</td><td>972</td></tr><tr><td>17</td><td>959</td><td>954</td></tr><tr><td>18</td><td>966</td><td>975</td></tr><tr><td>19</td><td>961</td><td>969</td></tr><tr><td>20</td><td>964</td><td>963</td></tr><tr><td>21</td><td>965</td><td>962</td></tr><tr><td>22</td><td>962</td><td>933</td></tr><tr><td>23</td><td>962</td><td>993</td></tr><tr><td>24</td><td>958</td><td>961</td></tr><tr><td>25</td><td>967</td><td>958</td></tr><tr><td>26</td><td>963</td><td>958</td></tr><tr><td>27</td><td>961</td><td>956</td></tr><tr><td>28</td><td>963</td><td>970</td></tr><tr><td>29</td><td>965</td><td>962</td></tr><tr><td>30</td><td>962</td><td>963</td></tr></tbody></table><p>Surprisingly, both achieve the desired bandwith (1Gbps) for the selected VM sizes.</p><p>I was still curious if the performance profile was the same when upgrading both VMs to support 10Gbps networking. For this test both machines were upgraded to the DS14v2 VM size. To maximise the bandwidth I used iperfs <code>-P</code> switch to run concurrent workers. The buffer size was also increased to see the effect it has on the bandwidth.</p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="comment">#Public IP with 4 processes</span></div><div class="line">.\iperf3.exe -c 13.7xx.xx.xx -i 1 -t 30 -P 4</div><div class="line"><span class="comment">#Peered network with 4 processes</span></div><div class="line">.\iperf3.exe -c 10.xx.xx.xx -i 1 -t 30 -P 4</div><div class="line"><span class="comment">#Public IP with 4 processes and 32MB buffer</span></div><div class="line">.\iperf3.exe -c 13.xx.xx.xx -i 1 -t 30 -P 4 -w 32MB</div><div class="line"><span class="comment">#Peered network with 4 processes and 32MB buffer</span></div><div class="line">.\iperf3.exe -c 10.xx.xx.xx -i 1 -t 30 -P 4 -w 32MB</div></pre></td></tr></table></figure><chart type="BarChart" options="{'hAxis':{'baseline':0}, 'legend':{'position':'bottom'}, 'height':'300'}"><br><div></div><br></chart><table><thead><tr><th>Test</th><th>Bandwidth (Mbps)</th></tr></thead><tbody><tr><td>Public IP</td><td>2480</td></tr><tr><td>Peered</td><td>2630</td></tr><tr><td>Public IP (32MB)</td><td>3230</td></tr><tr><td>Peered (32MB)</td><td>2710</td></tr></tbody></table><p>As expected, with the default values the peered network performed better although the difference was marginal. More surprisingly, the public network had a high thoroughput when the buffer size was increased and despite running the test multiple times I am unable to explain why.</p><p>For our workload and use case, I can say the performance difference between the two approaches is irrelevant. If you are evaluating whether you might gain network performance by switching to peered networking then I hope these numbers can help guide you. I would recommend running a similar test if you are choosing different VM sizes or workload.</p><p></p><div class="categories"><span>Posted in: </span><a href="/blog/categories/operations/">operations</a></div><div class="tags"><span>Tagged with: </span><a href="/blog/tags/azure/">azure</a></div><br><hr><div class="row"><div class="col-md-12"><h3>Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div></div></div></div><div id="push"></div></div><div id="footer"><div class="container"><div class="container"><br><p class="muted credit">&copy; 2016 Naeem Khedarun</p></div></div></div><script src="/js/all-8d7c4a0adc.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-71815871-1', 'auto');
ga('send', 'pageview');</script><script type="text/javascript">var disqus_shortname = 'naeemkhedarun';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());</script></body></html>